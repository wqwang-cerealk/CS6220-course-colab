{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4e68dd8f-aab6-4969-be1c-85e3958eb5ca",
      "metadata": {
        "id": "4e68dd8f-aab6-4969-be1c-85e3958eb5ca"
      },
      "source": [
        "# Latent Dirichlet Allocation (LDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "02b7c30d-baa6-4186-a98a-f090f82e38a2",
      "metadata": {
        "id": "02b7c30d-baa6-4186-a98a-f090f82e38a2"
      },
      "source": [
        "This Colab is taken and modified from [here](https://colab.research.google.com/github/dudaspm/LDA_Bias_Data/blob/main/Latent%20Dirichlet%20Allocation%20(LDA).ipynb)\n",
        "\n",
        "<center>\n",
        "<img src=\"https://www.gutenberg.org/files/55/55-h/images/cover.jpg\"  width=\"300\"></img>\n",
        "</center>\n",
        "\n",
        "The Wonderful Wizard of Oz via https://www.gutenberg.org/ebooks/55"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d6bab6c7-8078-483b-a84a-dec916201024",
      "metadata": {
        "id": "d6bab6c7-8078-483b-a84a-dec916201024"
      },
      "source": [
        "* What are topics? \n",
        "    * The topics will X number of sets of terms (we define this X) which will (could) have a common theme. \n",
        "* How are they defined? \n",
        "    * This is what we will get to in this notebook.     \n",
        "* Do we define or does the computer? \n",
        "    * LDA is unsupervised, so we define the number of topics. The computer provides the topics themselves. \n",
        "* What is a large corpus? and How many documents do we need? \n",
        "    * A bit subjective here. There is a *great* discussion about this by Tang et al.  {cite:p}`tang2014understanding` regarding this. If you have a chance, read all the points, but to sum it up\n",
        "        * The number of documents does matter, but at some point, increasing the number does not warrant better results. Even sampling 1000 papers from 1000000 papers could result in the same, if not better, results than 1000000 documents. \n",
        "        * The size of the documents also plays a role, so documents should not be short. Larger documents can be sampled and again receive the same desired output. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "faf3ca1c-1dc5-4f33-acfc-9e26c3f2c73b",
      "metadata": {
        "id": "faf3ca1c-1dc5-4f33-acfc-9e26c3f2c73b"
      },
      "source": [
        "For more information about LDA, please review [this high level article](https://www.cs.columbia.edu/~blei/papers/Blei2012.pdf). If interested, review at my office hours, and consider taking CS6120: Natural Language Processing. In this course, we will be applying LDA with some base understandings of the approach.\n",
        "\n",
        "<center>\n",
        "<img src=\"http://deliveryimages.acm.org/10.1145/2140000/2133826/figs/f1.jpg\"  width=\"600\"></img>\n",
        "</center>\n",
        "Figure 1. The intuitions behind latent Dirichlet allocation. We assume that some number of \"topics,\" which are distributions over words, exist for the whole collection (far left). Each document is assumed to be generated as follows. First choose a distribution over the topics (the histogram at right); then, for each word, choose a topic assignment (the colored coins) and choose the word from the corresponding topic. The topics and topic assignments in this figure are illustrativeâ€”they are not fit from real data. (Page 3)\n",
        "\n",
        "\n",
        "<center>\n",
        "<img src=\"https://deliveryimages.acm.org/10.1145/2140000/2133826/figs/f2.jpg\"  width=\"600\"></img>\n",
        "</center>\n",
        "Figure 2. Real inference with LDA. We fit a 100-topic LDA model to 17,000 articles from the journal Science. At left are the inferred topic proportions for the example article in Figure 1. At right are the top 15 most frequent words from the most frequent topics found in this article."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# %%capture\n",
        "!pip install sklearn"
      ],
      "metadata": {
        "id": "OUw6TpOZUJYD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a9409b7-7b71-4b2c-a650-b0a1559c94b1"
      },
      "id": "OUw6TpOZUJYD",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting sklearn\n",
            "  Downloading sklearn-0.0.post1.tar.gz (3.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: sklearn\n",
            "  Building wheel for sklearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sklearn: filename=sklearn-0.0.post1-py3-none-any.whl size=2344 sha256=f1e490515c159f406ea3295117a62d93e95e3800d1d8aec6466f6e764114da94\n",
            "  Stored in directory: /root/.cache/pip/wheels/14/25/f7/1cc0956978ae479e75140219088deb7a36f60459df242b1a72\n",
            "Successfully built sklearn\n",
            "Installing collected packages: sklearn\n",
            "Successfully installed sklearn-0.0.post1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4dbe6f0-e011-4ca9-b564-de59f28159b0",
      "metadata": {
        "id": "c4dbe6f0-e011-4ca9-b564-de59f28159b0"
      },
      "source": [
        "## Let's Try an Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc077908-6bad-4e1c-80e1-161ba6a4458f",
      "metadata": {
        "id": "fc077908-6bad-4e1c-80e1-161ba6a4458f"
      },
      "outputs": [],
      "source": [
        "from urllib.request import urlopen\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# The Wonderful Wizard of Oz\n",
        "url = \"https://www.gutenberg.org/files/55/55-h/55-h.htm\" \n",
        "html = urlopen(url).read()\n",
        "soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "# Remove CSS (style) or Javascript (script)\n",
        "for script in soup([\"script\", \"style\"]):\n",
        "    script.extract()\n",
        "text = soup.get_text()\n",
        "documents = []\n",
        "documents.append(text)\n",
        "\n",
        "# The Marvellous Land of Oz\n",
        "url = \"https://www.gutenberg.org/files/54/54-h/54-h.htm\" \n",
        "html = urlopen(url).read()\n",
        "soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "for script in soup([\"script\", \"style\"]):\n",
        "    script.extract()\n",
        "text = soup.get_text()\n",
        "documents.append(text)\n",
        "\n",
        "# Ozma of Oz\n",
        "url = \"https://www.gutenberg.org/files/33361/33361-h/33361-h.htm\" \n",
        "html = urlopen(url).read()\n",
        "soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "for script in soup([\"script\", \"style\"]):\n",
        "    script.extract()\n",
        "text = soup.get_text()\n",
        "documents.append(text)\n",
        "\n",
        "# Dorothy and the Wizard of Oz\n",
        "url = \"https://www.gutenberg.org/files/22566/22566-h/22566-h.htm\" \n",
        "html = urlopen(url).read()\n",
        "soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "for script in soup([\"script\", \"style\"]):\n",
        "    script.extract()\n",
        "text = soup.get_text()\n",
        "documents.append(text)\n",
        "\n",
        "# The Road to Oz\n",
        "url = \"https://www.gutenberg.org/files/26624/26624-h/26624-h.htm\" \n",
        "html = urlopen(url).read()\n",
        "soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "for script in soup([\"script\", \"style\"]):\n",
        "    script.extract()\n",
        "text = soup.get_text()\n",
        "documents.append(text)\n",
        "\n",
        "# Earliest Years at Vassar\n",
        "url = \"https://www.gutenberg.org/cache/epub/46080/pg46080-images.html\" \n",
        "html = urlopen(url).read()\n",
        "soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "for script in soup([\"script\", \"style\"]):\n",
        "    script.extract()\n",
        "text = soup.get_text()\n",
        "documents.append(text)\n",
        "\n",
        "# Days in Queensland\n",
        "url = \"https://www.gutenberg.org/cache/epub/38649/pg38649-images.html\" \n",
        "html = urlopen(url).read()\n",
        "soup = BeautifulSoup(html, features=\"html.parser\")\n",
        "for script in soup([\"script\", \"style\"]):\n",
        "    script.extract()\n",
        "text = soup.get_text()\n",
        "documents.append(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "79da5c65-d8e6-4687-b9dd-8432035b9234",
      "metadata": {
        "id": "79da5c65-d8e6-4687-b9dd-8432035b9234"
      },
      "source": [
        "### Create Tokens and Vocabulary\n",
        "\n",
        "Now that we have our books, we need to tokenize the stories by word and then create a vocabulary out of these tokens. Note that we eliminate extremely common words that do not contribute much to the meaning of a document and topic (like `the`, `and`, `or`, etc.). These are called *stop words*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a7db0482-3d3e-4530-ac72-2a4fdb658c89",
      "metadata": {
        "id": "a7db0482-3d3e-4530-ac72-2a4fdb658c89",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e816cde1-699b-45e3-e743-8dcc7d25aafe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
            "  warnings.warn(msg, category=FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "cv = CountVectorizer(stop_words='english')\n",
        "df = cv.fit_transform(documents)\n",
        "vocab = cv.get_feature_names()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b83a8cb3-8446-4e1b-b4e8-08f35e4ddbd6",
      "metadata": {
        "id": "b83a8cb3-8446-4e1b-b4e8-08f35e4ddbd6"
      },
      "source": [
        "Let's take a look at the tokens and the number of occurrence for the tokens. \n",
        "\n",
        "Question: What do the dimensions in df mean?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "010f147b-256f-4d79-aafb-28cb0ed3aa65",
      "metadata": {
        "id": "010f147b-256f-4d79-aafb-28cb0ed3aa65",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "578cf5b8-62a0-46a4-eeb0-b6b23fed5172"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  (0, 9720)\t89\n",
            "  (0, 5998)\t99\n",
            "  (0, 4342)\t14\n",
            "  (0, 13956)\t28\n",
            "  (0, 13933)\t44\n",
            "  (0, 8868)\t169\n",
            "  (0, 5434)\t5\n",
            "  (0, 1431)\t5\n",
            "  (0, 13349)\t28\n",
            "  (0, 13234)\t15\n",
            "  (0, 11900)\t19\n",
            "  (0, 8980)\t2\n",
            "  (0, 13999)\t19\n",
            "  (0, 3170)\t4\n",
            "  (0, 10458)\t2\n",
            "  (0, 13772)\t2\n",
            "  (0, 3122)\t12\n",
            "  (0, 1278)\t71\n",
            "  (0, 12552)\t22\n",
            "  (0, 7428)\t18\n",
            "  (0, 6639)\t3\n",
            "  (0, 8693)\t4\n",
            "  (0, 14057)\t10\n",
            "  (0, 8741)\t10\n",
            "  (0, 7540)\t7\n",
            "  :\t:\n",
            "  (0, 8888)\t1\n",
            "  (0, 4095)\t1\n",
            "  (0, 8013)\t1\n",
            "  (0, 639)\t1\n",
            "  (0, 13690)\t1\n",
            "  (0, 3279)\t1\n",
            "  (0, 2231)\t1\n",
            "  (0, 9701)\t1\n",
            "  (0, 8018)\t1\n",
            "  (0, 6119)\t1\n",
            "  (0, 8758)\t1\n",
            "  (0, 11172)\t1\n",
            "  (0, 9691)\t1\n",
            "  (0, 7584)\t1\n",
            "  (0, 8443)\t1\n",
            "  (0, 13543)\t1\n",
            "  (0, 2938)\t1\n",
            "  (0, 8397)\t1\n",
            "  (0, 4360)\t1\n",
            "  (0, 7711)\t1\n",
            "  (0, 9201)\t1\n",
            "  (0, 4918)\t1\n",
            "  (0, 9690)\t1\n",
            "  (0, 12159)\t1\n",
            "  (0, 8454)\t1\n"
          ]
        }
      ],
      "source": [
        "# PLAY AROUND WITH `df` HERE\n",
        "print(df[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f93c6b19-5e3d-4a83-9c5d-4d6518aaf4af",
      "metadata": {
        "id": "f93c6b19-5e3d-4a83-9c5d-4d6518aaf4af"
      },
      "source": [
        "In the variable `df`, the second number listed is the token number, and we use the vocab list to see what the actual word. An example would be to look at the first line. \n",
        "\n",
        "```python\n",
        "(0, 8074) 3198\n",
        "```\n",
        "The 8074 token was used 3198 times. The 8074 token is:\n",
        "\n",
        "Question: What word/vocab does token 8074 correspond to? How many times is it used? Is this surprising?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "820ce127-340f-4ebc-8437-8571a1ee23a8",
      "metadata": {
        "id": "820ce127-340f-4ebc-8437-8571a1ee23a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "388c6583-8b08-400d-e365-8b82beb5330b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'mins'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# YOUR CODE HERE\n",
        "vocab[8074]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa51075a-6cc9-4bcb-b136-951e622a44d8",
      "metadata": {
        "id": "fa51075a-6cc9-4bcb-b136-951e622a44d8"
      },
      "source": [
        "From here, we are actually at the point where we can run LDA.\n",
        "\n",
        "There are much more than two inputs available for LDA, but we will focus on two. \n",
        "> Here are the other inputs: https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html\n",
        "\n",
        "The two we will focus on are:\n",
        "\n",
        "* n_components - the number of topics, again, we need to specify this\n",
        "* doc_topic_prior - this relates the Dirichlet distribution (the next notebook goes into full detail about Dirichlet and how it relates to LDA. \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f00d8bf9-c910-49a7-b01b-f1fa3193f41a",
      "metadata": {
        "id": "f00d8bf9-c910-49a7-b01b-f1fa3193f41a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f59f6514-7f5e-4297-e022-688e4ebc5d9f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LatentDirichletAllocation(doc_topic_prior=1, n_components=5)"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "lda = LatentDirichletAllocation(n_components = 5, doc_topic_prior=1)\n",
        "\n",
        "# YOUR CODE HERE\n",
        "lda.fit(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7bab10a9-a092-4d2d-a5b1-0e4c9a5ebd24",
      "metadata": {
        "id": "7bab10a9-a092-4d2d-a5b1-0e4c9a5ebd24"
      },
      "source": [
        "To print out the top-5 words per topic, we used a solution from StackOverflow."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "070e0159-357b-40d3-be47-da86e8c90bb5",
      "metadata": {
        "id": "070e0159-357b-40d3-be47-da86e8c90bb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73a85a5d-5677-46ad-d88d-c38d97151629"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Topic: 0\n",
            "  gutenberg, college, project, miss, work, day, time, mitchell, students, vassar\n",
            "Topic: 1\n",
            "  said, dorothy, scarecrow, man, woodman, tin, little, asked, oz, tip\n",
            "Topic: 2\n",
            "  river, country, pg, mr, new, queensland, great, north, stock, water\n",
            "Topic: 3\n",
            "  pumpkinhead, husband, lesson, impression, tastes, depths, series, holiday, suited, lamb\n",
            "Topic: 4\n",
            "  dorothy, said, pg, little, wizard, king, ozma, girl, asked, gutenberg\n"
          ]
        }
      ],
      "source": [
        "import numpy as np \n",
        "topic_words = {}\n",
        "n_top_words = 10\n",
        "\n",
        "try:\n",
        "  for topic, comp in enumerate(lda.components_):\n",
        "      # for the n-dimensional array \"arr\":\n",
        "      # argsort() returns a ranked n-dimensional array of arr, call it \"ranked_array\"\n",
        "      # which contains the indices that would sort arr in a descending fashion\n",
        "      # for the ith element in ranked_array, ranked_array[i] represents the index of the\n",
        "      # element in arr that should be at the ith index in ranked_array\n",
        "      # ex. arr = [3,7,1,0,3,6]\n",
        "      # np.argsort(arr) -> [3, 2, 0, 4, 5, 1]\n",
        "      # word_idx contains the indices in \"topic\" of the top num_top_words most relevant\n",
        "      # to a given topic ... it is sorted ascending to begin with and then reversed (desc. now)    \n",
        "      word_idx = np.argsort(comp)[::-1][:n_top_words]\n",
        "\n",
        "      # store the words most relevant to the topic\n",
        "      topic_words[topic] = [vocab[i] for i in word_idx]\n",
        "      \n",
        "  for topic, words in topic_words.items():\n",
        "      print('Topic: %d' % topic)\n",
        "      print('  %s' % ', '.join(words))\n",
        "except:\n",
        "  print(\"Did you fit the data?\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}